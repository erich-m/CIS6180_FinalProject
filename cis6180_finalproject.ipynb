{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Description\n",
    "\n",
    "##  Background\n",
    "\n",
    "Eye tracking is a technology that is used to measure the movement and position of the eye. Eye tracking can be used to obtain a variety of information, such as where someone is looking (also known as the gaze point). The raw eye tracking data cann also be used to engineer new features - eye tracking events - which can further be used to obtain more information. \n",
    "\n",
    "The types of eye tracking events that we can measure for include fixations, which are periods of time where the eye fixates on a target. There are saccades where the eyes move between points of fixations. There are also post-saccidic oscillations and glissades where the eye will oscillate after a saccade before settling to a fixation point. Post-saccadic oscillations overshoot the target, while glissades undershoot.\n",
    "\n",
    "These types of events can be measured by applying different threshold techniques. I-VT applies a velocity threshold; If the speed between two gaze points is below a certain threshold, it is identified as a fixation. If the speed is above the threshold, it is a  saccade. There is also a dispersion/distance based method as well known as I-DT, that uses the distance between the gaze points instead to classify either fixations and saccades. These threshold algorithms are common in practice, but do not have the ability to classify more complex events. \n",
    "\n",
    "For the purpose of performing the I-VT  algorithm, a speed of 0.5px/ms was selected, and a dispersion of 1ยบ was selected for I-DT.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For the following notebook, the dataset used is from a study performed in the University of Guelph DRiVE lab. Particpants wore eye-tracking glasses (Tobii Pro 3 glasses) and drove an OKTAL driving simulator. The dataset contains 72 participants that are randomly separated into train, test and validation sets. This will prevent leakage amonst the different particpant data.  Each of the files contains 3 different sets of data. There is some device information that is read in and in the sheet titled 'Event Data'. There is IMU sensor data in the sheet titled 'IMU Data'. The eye tracking data is in the sheet titled 'Gaze Data'. The sheets have 4, 22 and 11 columns respectively. The data  from the eye tracker is collected at 60Hz, and each participant file has roughly 20000 records in each file. The data is pre-split to ensure that there is no leakage between participant data, which could affect the training of the models, and to ensure a more consistent evaluation of the performance of the models.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "1. The Gaze Data is read into the notebook using an Excel library.\n",
    "2. For each participant file, any gaps in the data are filled in using linear interpolation first. \n",
    "3. Next, every two records are taken to calculate the labels using I-VT and I-DT and these are stored into a new dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "# import spark libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dataset_training','dataset_testing','dataset_validation'] # directories for training, testing and valdiation\n",
    "sheet = 'Gaze Data' # name of sheet with the eye tracking data\n",
    "\n",
    "column_names = ['Type', 'Timestamp', 'Data_Gaze2D_X', 'Data_Gaze2D_Y', 'Data_Gaze3D_X',\n",
    "       'Data_Gaze3D_Y', 'Data_Gaze3D_Z', 'Data_Eyeleft_Gazeorigin_X',\n",
    "       'Data_Eyeleft_Gazeorigin_Y', 'Data_Eyeleft_Gazeorigin_Z',\n",
    "       'Data_Eyeleft_Gazedirection_X', 'Data_Eyeleft_Gazedirection_Y',\n",
    "       'Data_Eyeleft_Gazedirection_Z', 'Data_Eyeleft_Pupildiameter',\n",
    "       'Data_Eyeright_Gazeorigin_X', 'Data_Eyeright_Gazeorigin_Y',\n",
    "       'Data_Eyeright_Gazeorigin_Z', 'Data_Eyeright_Gazedirection_X',\n",
    "       'Data_Eyeright_Gazedirection_Y', 'Data_Eyeright_Gazedirection_Z',\n",
    "       'Data_Eyeright_Pupildiameter']\n",
    "\n",
    "# createempty dataframe variables\n",
    "df_train = None\n",
    "df_test = None\n",
    "df_validation = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Cis6180_FinalProject\")\\\n",
    "    .config(\"spark.driver.memory\", \"48g\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "    .config(\"spark.memory.offHeap.size\",\"10g\")\\\n",
    "    .config(\"spark.executor.memory\", \"48g\")\\\n",
    "    .config(\"spark.executor.cores\", 4)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 1)\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 4)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create List of Files to Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "\n",
    "# iterate through all the files in the dataset to ge\n",
    "for ds_num,dataset in enumerate(datasets):\n",
    "    data_files = listdir(dataset)\n",
    "    for f_num,f in enumerate(data_files):\n",
    "        file_path = dataset + '/' + f # file path is the relative file path for the current excel file\n",
    "        file_list.append((file_path,ds_num))\n",
    "\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Through All Files to Read into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 1/73 dataset_training/eye-data-10327.xlsx\n",
      "File: 2/73 dataset_training/eye-data-12471.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 3/73 dataset_training/eye-data-18514.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 4/73 dataset_training/eye-data-20116.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 5/73 dataset_training/eye-data-21051.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 6/73 dataset_training/eye-data-21895.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 7/73 dataset_training/eye-data-22013.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 8/73 dataset_training/eye-data-23090.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 9/73 dataset_training/eye-data-23753.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 10/73 dataset_training/eye-data-25462.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 11/73 dataset_training/eye-data-26370.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 12/73 dataset_training/eye-data-28334.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 13/73 dataset_training/eye-data-29048.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 14/73 dataset_training/eye-data-34473.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 15/73 dataset_training/eye-data-35217.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 16/73 dataset_training/eye-data-35745.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 17/73 dataset_training/eye-data-41517.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 18/73 dataset_training/eye-data-46121.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 19/73 dataset_training/eye-data-46307.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 20/73 dataset_training/eye-data-47274.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 21/73 dataset_training/eye-data-47402.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 22/73 dataset_training/eye-data-48737.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 23/73 dataset_training/eye-data-51637.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 24/73 dataset_training/eye-data-52063.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 25/73 dataset_training/eye-data-53209.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 26/73 dataset_training/eye-data-53349.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 27/73 dataset_training/eye-data-54455.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 28/73 dataset_training/eye-data-55367.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 29/73 dataset_training/eye-data-55746.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 30/73 dataset_training/eye-data-56135.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 31/73 dataset_training/eye-data-56233.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 32/73 dataset_training/eye-data-59774.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 33/73 dataset_training/eye-data-63923.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 34/73 dataset_training/eye-data-64765.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 35/73 dataset_training/eye-data-69876.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 36/73 dataset_training/eye-data-70253.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 37/73 dataset_training/eye-data-70615.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 38/73 dataset_training/eye-data-71291.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 39/73 dataset_training/eye-data-76001.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 40/73 dataset_training/eye-data-79820.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 41/73 dataset_training/eye-data-83008.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 42/73 dataset_training/eye-data-84384.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 43/73 dataset_training/eye-data-86812.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 44/73 dataset_training/eye-data-91060.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 45/73 dataset_training/eye-data-94231.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 46/73 dataset_training/eye-data-95397.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 47/73 dataset_training/eye-data-95985.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 48/73 dataset_training/eye-data-96194.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 49/73 dataset_training/eye-data-96679.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 50/73 dataset_training/eye-data-97448.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 51/73 dataset_training/eye-data-97973.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 52/73 dataset_testing/eye-data-11868.xlsx\n",
      "File: 53/73 dataset_testing/eye-data-21182.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 54/73 dataset_testing/eye-data-22446.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 55/73 dataset_testing/eye-data-23921.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 56/73 dataset_testing/eye-data-38989.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 57/73 dataset_testing/eye-data-46094.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 58/73 dataset_testing/eye-data-54097.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 59/73 dataset_testing/eye-data-72799.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 60/73 dataset_testing/eye-data-75601.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 61/73 dataset_testing/eye-data-91260.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 62/73 dataset_testing/eye-data-97051.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 63/73 dataset_validation/eye-data-11085.xlsx\n",
      "File: 64/73 dataset_validation/eye-data-14732.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 65/73 dataset_validation/eye-data-17381.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 66/73 dataset_validation/eye-data-19733.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 67/73 dataset_validation/eye-data-26585.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 68/73 dataset_validation/eye-data-29097.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 69/73 dataset_validation/eye-data-37883.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 70/73 dataset_validation/eye-data-39692.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 71/73 dataset_validation/eye-data-41473.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 72/73 dataset_validation/eye-data-51553.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 73/73 dataset_validation/eye-data-64087.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8890: FutureWarning: The DataFrame.append method is deprecated and will be removed in 4.0.0. Use pyspark.pandas.concat instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_num = 1\n",
    "\n",
    "# iterate through the list of files to read them, interpolate missing data, merge them and convert to pyspark\n",
    "# this takes 5-20 minutes and should be optimized in the future\n",
    "\n",
    "for file_path in file_list:\n",
    "    print(f'File: {file_num}/{len(file_list)} {file_path[0]}')\n",
    "    \n",
    "    # read the dataframe as python and then convert to pyspark since it has to be read in from excel spreadsheet\n",
    "    ppdf = pd.read_excel(io=file_path[0],sheet_name=sheet) # read excel as pandas\n",
    "    ppdf.columns = column_names\n",
    "    ppdf = ppdf.drop(['Type'],axis=1)\n",
    "    ppdf = ppdf.interpolate(method='linear',limit_direction='both')\n",
    "\n",
    "    ppdf_first = ppdf.iloc[:-1] # create copy of ppdf and remove last row\n",
    "    ppdf_second = ppdf.copy().iloc[1:] # create a copy of ppdf and remove the first row\n",
    "    ppdf_second.reset_index(drop=True, inplace=True)  # Reset index to ensure alignment\n",
    "\n",
    "    ppdf_second.columns = [col + '_2' for col in ppdf_second.columns]# rename the columns of the second copy\n",
    "    ppdf_combined = pd.concat([ppdf_first,ppdf_second],axis=1) # merge first copy and second copy side by side\n",
    "\n",
    "    psdf = ps.from_pandas(ppdf_combined)\n",
    "    \n",
    "    if file_path[1] == 0:  # train\n",
    "        if df_train is None:\n",
    "            df_train = psdf\n",
    "        else:\n",
    "            df_train.append(psdf, ignore_index = True)# append current dataset to the one that is already existing\n",
    "\n",
    "    elif file_path[1] == 1:  # test\n",
    "        if df_test is None:\n",
    "            df_test = psdf\n",
    "        else:\n",
    "            df_test.append(psdf, ignore_index = True)# append current dataset to the one that is already existing\n",
    "    elif file_path[1] == 2:  # validation\n",
    "        if df_validation is None:\n",
    "            df_validation = psdf\n",
    "        else:\n",
    "            df_validation.append(psdf, ignore_index = True)\n",
    "\n",
    "    file_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp                          float64\n",
       "Data_Gaze2D_X                      float64\n",
       "Data_Gaze2D_Y                      float64\n",
       "Data_Gaze3D_X                      float64\n",
       "Data_Gaze3D_Y                      float64\n",
       "Data_Gaze3D_Z                      float64\n",
       "Data_Eyeleft_Gazeorigin_X          float64\n",
       "Data_Eyeleft_Gazeorigin_Y          float64\n",
       "Data_Eyeleft_Gazeorigin_Z          float64\n",
       "Data_Eyeleft_Gazedirection_X       float64\n",
       "Data_Eyeleft_Gazedirection_Y       float64\n",
       "Data_Eyeleft_Gazedirection_Z       float64\n",
       "Data_Eyeleft_Pupildiameter         float64\n",
       "Data_Eyeright_Gazeorigin_X         float64\n",
       "Data_Eyeright_Gazeorigin_Y         float64\n",
       "Data_Eyeright_Gazeorigin_Z         float64\n",
       "Data_Eyeright_Gazedirection_X      float64\n",
       "Data_Eyeright_Gazedirection_Y      float64\n",
       "Data_Eyeright_Gazedirection_Z      float64\n",
       "Data_Eyeright_Pupildiameter        float64\n",
       "Timestamp_2                        float64\n",
       "Data_Gaze2D_X_2                    float64\n",
       "Data_Gaze2D_Y_2                    float64\n",
       "Data_Gaze3D_X_2                    float64\n",
       "Data_Gaze3D_Y_2                    float64\n",
       "Data_Gaze3D_Z_2                    float64\n",
       "Data_Eyeleft_Gazeorigin_X_2        float64\n",
       "Data_Eyeleft_Gazeorigin_Y_2        float64\n",
       "Data_Eyeleft_Gazeorigin_Z_2        float64\n",
       "Data_Eyeleft_Gazedirection_X_2     float64\n",
       "Data_Eyeleft_Gazedirection_Y_2     float64\n",
       "Data_Eyeleft_Gazedirection_Z_2     float64\n",
       "Data_Eyeleft_Pupildiameter_2       float64\n",
       "Data_Eyeright_Gazeorigin_X_2       float64\n",
       "Data_Eyeright_Gazeorigin_Y_2       float64\n",
       "Data_Eyeright_Gazeorigin_Z_2       float64\n",
       "Data_Eyeright_Gazedirection_X_2    float64\n",
       "Data_Eyeright_Gazedirection_Y_2    float64\n",
       "Data_Eyeright_Gazedirection_Z_2    float64\n",
       "Data_Eyeright_Pupildiameter_2      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Data_Gaze2D_X', 'Data_Gaze2D_Y', 'Data_Gaze3D_X',\n",
       "       'Data_Gaze3D_Y', 'Data_Gaze3D_Z', 'Data_Eyeleft_Gazeorigin_X',\n",
       "       'Data_Eyeleft_Gazeorigin_Y', 'Data_Eyeleft_Gazeorigin_Z',\n",
       "       'Data_Eyeleft_Gazedirection_X', 'Data_Eyeleft_Gazedirection_Y',\n",
       "       'Data_Eyeleft_Gazedirection_Z', 'Data_Eyeleft_Pupildiameter',\n",
       "       'Data_Eyeright_Gazeorigin_X', 'Data_Eyeright_Gazeorigin_Y',\n",
       "       'Data_Eyeright_Gazeorigin_Z', 'Data_Eyeright_Gazedirection_X',\n",
       "       'Data_Eyeright_Gazedirection_Y', 'Data_Eyeright_Gazedirection_Z',\n",
       "       'Data_Eyeright_Pupildiameter', 'Timestamp_2', 'Data_Gaze2D_X_2',\n",
       "       'Data_Gaze2D_Y_2', 'Data_Gaze3D_X_2', 'Data_Gaze3D_Y_2',\n",
       "       'Data_Gaze3D_Z_2', 'Data_Eyeleft_Gazeorigin_X_2',\n",
       "       'Data_Eyeleft_Gazeorigin_Y_2', 'Data_Eyeleft_Gazeorigin_Z_2',\n",
       "       'Data_Eyeleft_Gazedirection_X_2', 'Data_Eyeleft_Gazedirection_Y_2',\n",
       "       'Data_Eyeleft_Gazedirection_Z_2', 'Data_Eyeleft_Pupildiameter_2',\n",
       "       'Data_Eyeright_Gazeorigin_X_2', 'Data_Eyeright_Gazeorigin_Y_2',\n",
       "       'Data_Eyeright_Gazeorigin_Z_2', 'Data_Eyeright_Gazedirection_X_2',\n",
       "       'Data_Eyeright_Gazedirection_Y_2', 'Data_Eyeright_Gazedirection_Z_2',\n",
       "       'Data_Eyeright_Pupildiameter_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33182.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 21) (ER1CHS-LAPT0P executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\generic.py:1795\u001b[0m, in \u001b[0;36mFrame.count\u001b[1;34m(self, axis, numeric_only)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\n\u001b[0;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m, axis: Optional[Axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, numeric_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1727\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Scalar, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;124;03m    Count non-NA cells for each column.\u001b[39;00m\n\u001b[0;32m   1730\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_for_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:810\u001b[0m, in \u001b[0;36mDataFrame._reduce_for_stat_function\u001b[1;34m(self, sfun, name, axis, numeric_only, skipna, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ps\u001b[38;5;241m.\u001b[39moption_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute.max_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    804\u001b[0m         internal \u001b[38;5;241m=\u001b[39m InternalFrame(\n\u001b[0;32m    805\u001b[0m             spark_frame\u001b[38;5;241m=\u001b[39msdf,\n\u001b[0;32m    806\u001b[0m             index_spark_columns\u001b[38;5;241m=\u001b[39m[scol_for(sdf, SPARK_DEFAULT_INDEX_NAME)],\n\u001b[0;32m    807\u001b[0m             column_labels\u001b[38;5;241m=\u001b[39mnew_column_labels,\n\u001b[0;32m    808\u001b[0m             column_label_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal\u001b[38;5;241m.\u001b[39mcolumn_label_names,\n\u001b[0;32m    809\u001b[0m         )\n\u001b[1;32m--> 810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m first_series(\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;66;03m# Here we execute with the first 1000 to get the return type.\u001b[39;00m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;66;03m# If the records were less than 1000, it uses pandas API directly for a shortcut.\u001b[39;00m\n\u001b[0;32m    815\u001b[0m     limit \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute.shortcut_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:2722\u001b[0m, in \u001b[0;36mDataFrame.transpose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2720\u001b[0m max_compute_count \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute.max_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_compute_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2722\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_compute_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m>\u001b[39m max_compute_count:\n\u001b[0;32m   2724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2725\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent DataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms length exceeds the given limit of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m rows. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m by using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyspark.pandas.config.set_option\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2730\u001b[0m             )\n\u001b[0;32m   2731\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  13382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m  13383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  13384\u001b[0m \u001b[38;5;124;03m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[0;32m  13385\u001b[0m \n\u001b[0;32m  13386\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[0;32m  13387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 13388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m--> 600\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[1;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1117\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m   1118\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[0;32m   1119\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \n\u001b[0;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o33182.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 21) (ER1CHS-LAPT0P executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "df_train.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
