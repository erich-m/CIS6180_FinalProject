{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Description\n",
    "\n",
    "##  Background\n",
    "\n",
    "Eye tracking is a technology that is used to measure the movement and position of the eye. Eye tracking can be used to obtain a variety of information, such as where someone is looking (also known as the gaze point). The raw eye tracking data cann also be used to engineer new features - eye tracking events - which can further be used to obtain more information. \n",
    "\n",
    "The types of eye tracking events that we can measure for include fixations, which are periods of time where the eye fixates on a target. There are saccades where the eyes move between points of fixations. There are also post-saccidic oscillations and glissades where the eye will oscillate after a saccade before settling to a fixation point. Post-saccadic oscillations overshoot the target, while glissades undershoot.\n",
    "\n",
    "These types of events can be measured by applying different threshold techniques. I-VT applies a velocity threshold; If the speed between two gaze points is below a certain threshold, it is identified as a fixation. If the speed is above the threshold, it is a  saccade. There is also a dispersion/distance based method as well known as I-DT, that uses the distance between the gaze points instead to classify either fixations and saccades. These threshold algorithms are common in practice, but do not have the ability to classify more complex events. \n",
    "\n",
    "For the purpose of performing the I-VT  algorithm, a speed of 0.5px/ms was selected, and a dispersion of 1ยบ was selected for I-DT.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For the following notebook, the dataset used is from a study performed in the University of Guelph DRiVE lab. Particpants wore eye-tracking glasses (Tobii Pro 3 glasses) and drove an OKTAL driving simulator. The dataset contains 72 participants that are randomly separated into train, test and validation sets. This will prevent leakage amonst the different particpant data.  Each of the files contains 3 different sets of data. There is some device information that is read in and in the sheet titled 'Event Data'. There is IMU sensor data in the sheet titled 'IMU Data'. The eye tracking data is in the sheet titled 'Gaze Data'. The sheets have 4, 22 and 11 columns respectively. The data  from the eye tracker is collected at 60Hz, and each participant file has roughly 20000 records in each file. The data is pre-split to ensure that there is no leakage between participant data, which could affect the training of the models, and to ensure a more consistent evaluation of the performance of the models.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "1. The Gaze Data is read into the notebook using an Excel library.\n",
    "2. For each participant file, any gaps in the data are filled in using linear interpolation first. \n",
    "3. Next, every two records are taken to calculate the labels using I-VT and I-DT and these are stored into a new dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "# import spark libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dataset_training','dataset_testing','dataset_validation'] # directories for training, testing and valdiation\n",
    "sheet = 'Gaze Data' # name of sheet with the eye tracking data\n",
    "\n",
    "column_names = ['Type', 'Timestamp', 'Data_Gaze2D_X', 'Data_Gaze2D_Y', 'Data_Gaze3D_X',\n",
    "       'Data_Gaze3D_Y', 'Data_Gaze3D_Z', 'Data_Eyeleft_Gazeorigin_X',\n",
    "       'Data_Eyeleft_Gazeorigin_Y', 'Data_Eyeleft_Gazeorigin_Z',\n",
    "       'Data_Eyeleft_Gazedirection_X', 'Data_Eyeleft_Gazedirection_Y',\n",
    "       'Data_Eyeleft_Gazedirection_Z', 'Data_Eyeleft_Pupildiameter',\n",
    "       'Data_Eyeright_Gazeorigin_X', 'Data_Eyeright_Gazeorigin_Y',\n",
    "       'Data_Eyeright_Gazeorigin_Z', 'Data_Eyeright_Gazedirection_X',\n",
    "       'Data_Eyeright_Gazedirection_Y', 'Data_Eyeright_Gazedirection_Z',\n",
    "       'Data_Eyeright_Pupildiameter']\n",
    "\n",
    "# createempty dataframe variables\n",
    "df_train = None\n",
    "df_test = None\n",
    "df_validation = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Cis6180_FinalProject\")\\\n",
    "    .config(\"spark.driver.memory\", \"48g\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "    .config(\"spark.memory.offHeap.size\",\"10g\")\\\n",
    "    .config(\"spark.executor.memory\", \"48g\")\\\n",
    "    .config(\"spark.executor.cores\", 4)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 1)\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 4)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create List of Files to Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "\n",
    "# iterate through all the files in the dataset to ge\n",
    "for ds_num,dataset in enumerate(datasets):\n",
    "    data_files = listdir(dataset)\n",
    "    for f_num,f in enumerate(data_files):\n",
    "        file_path = dataset + '/' + f # file path is the relative file path for the current excel file\n",
    "        file_list.append((file_path,ds_num))\n",
    "\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Through All Files to Read into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 1/73 dataset_training/eye-data-10327.xlsx\n",
      "Shape pre copy and merge: (26794, 21)\n",
      "Shape post copy and merge: (26793, 40)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o113.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (ER1CHS-LAPT0P executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape post copy and merge: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppdf_combined\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# this is fine\u001b[39;00m\n\u001b[0;32m     27\u001b[0m psdf \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mfrom_pandas(ppdf_combined)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpySpark DF shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpsdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:8290\u001b[0m, in \u001b[0;36mDataFrame.shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   8274\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   8275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   8276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8277\u001b[0m \u001b[38;5;124;03m    Return a tuple representing the dimensionality of the DataFrame.\u001b[39;00m\n\u001b[0;32m   8278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8288\u001b[0m \u001b[38;5;124;03m    (2, 3)\u001b[39;00m\n\u001b[0;32m   8289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 8290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\pandas\\frame.py:13589\u001b[0m, in \u001b[0;36mDataFrame.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  13588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m> 13589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolved_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\01eco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o113.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (ER1CHS-LAPT0P executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "file_num = 1\n",
    "\n",
    "# iterate through the list of files to read them, interpolate missing data, merge them and convert to pyspark\n",
    "# this takes 5-20 minutes and should be optimized in the future\n",
    "\n",
    "for file_path in file_list:\n",
    "    print(f'File: {file_num}/{len(file_list)} {file_path[0]}')\n",
    "    \n",
    "    # read the dataframe as python and then convert to pyspark since it has to be read in from excel spreadsheet\n",
    "    ppdf = pd.read_excel(io=file_path[0],sheet_name=sheet) # read excel as pandas\n",
    "\n",
    "    print(f'Shape pre copy and merge: {ppdf.shape}')# this is fine\n",
    "\n",
    "    ppdf.columns = column_names\n",
    "    ppdf = ppdf.drop(['Type'],axis=1)\n",
    "    ppdf = ppdf.interpolate(method='linear',limit_direction='both')\n",
    "\n",
    "    ppdf_first = ppdf.iloc[:-1] # create copy of ppdf and remove last row\n",
    "    ppdf_second = ppdf.copy().iloc[1:] # create a copy of ppdf and remove the first row\n",
    "    ppdf_second.reset_index(drop=True, inplace=True)  # Reset index to ensure alignment\n",
    "\n",
    "    ppdf_second.columns = [col + '_2' for col in ppdf_second.columns]# rename the columns of the second copy\n",
    "    ppdf_combined = pd.concat([ppdf_first,ppdf_second],axis=1) # merge first copy and second copy side by side\n",
    "\n",
    "    print(f'Shape post copy and merge: {ppdf_combined.shape}')# this is fine\n",
    "\n",
    "    psdf = ps.from_pandas(ppdf_combined) # this does not work\n",
    "\n",
    "    print(f'pySpark DF shape: {psdf.shape}') # this does not work\n",
    "    \n",
    "    if file_path[1] == 0:  # train\n",
    "        if df_train is None:\n",
    "            df_train = psdf\n",
    "        else:\n",
    "            df_train.append(psdf, ignore_index = True)# append current dataset to the one that is already existing\n",
    "\n",
    "    elif file_path[1] == 1:  # test\n",
    "        if df_test is None:\n",
    "            df_test = psdf\n",
    "        else:\n",
    "            df_test.append(psdf, ignore_index = True)# append current dataset to the one that is already existing\n",
    "    elif file_path[1] == 2:  # validation\n",
    "        if df_validation is None:\n",
    "            df_validation = psdf\n",
    "        else:\n",
    "            df_validation.append(psdf, ignore_index = True)\n",
    "\n",
    "    file_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Data_Gaze2D_X</th>\n",
       "      <th>Data_Gaze2D_Y</th>\n",
       "      <th>Data_Gaze3D_X</th>\n",
       "      <th>Data_Gaze3D_Y</th>\n",
       "      <th>Data_Gaze3D_Z</th>\n",
       "      <th>Data_Eyeleft_Gazeorigin_X</th>\n",
       "      <th>Data_Eyeleft_Gazeorigin_Y</th>\n",
       "      <th>Data_Eyeleft_Gazeorigin_Z</th>\n",
       "      <th>Data_Eyeleft_Gazedirection_X</th>\n",
       "      <th>Data_Eyeleft_Gazedirection_Y</th>\n",
       "      <th>Data_Eyeleft_Gazedirection_Z</th>\n",
       "      <th>Data_Eyeleft_Pupildiameter</th>\n",
       "      <th>Data_Eyeright_Gazeorigin_X</th>\n",
       "      <th>Data_Eyeright_Gazeorigin_Y</th>\n",
       "      <th>Data_Eyeright_Gazeorigin_Z</th>\n",
       "      <th>Data_Eyeright_Gazedirection_X</th>\n",
       "      <th>Data_Eyeright_Gazedirection_Y</th>\n",
       "      <th>Data_Eyeright_Gazedirection_Z</th>\n",
       "      <th>Data_Eyeright_Pupildiameter</th>\n",
       "      <th>Timestamp_2</th>\n",
       "      <th>Data_Gaze2D_X_2</th>\n",
       "      <th>Data_Gaze2D_Y_2</th>\n",
       "      <th>Data_Gaze3D_X_2</th>\n",
       "      <th>Data_Gaze3D_Y_2</th>\n",
       "      <th>Data_Gaze3D_Z_2</th>\n",
       "      <th>Data_Eyeleft_Gazeorigin_X_2</th>\n",
       "      <th>Data_Eyeleft_Gazeorigin_Y_2</th>\n",
       "      <th>Data_Eyeleft_Gazeorigin_Z_2</th>\n",
       "      <th>Data_Eyeleft_Gazedirection_X_2</th>\n",
       "      <th>Data_Eyeleft_Gazedirection_Y_2</th>\n",
       "      <th>Data_Eyeleft_Gazedirection_Z_2</th>\n",
       "      <th>Data_Eyeleft_Pupildiameter_2</th>\n",
       "      <th>Data_Eyeright_Gazeorigin_X_2</th>\n",
       "      <th>Data_Eyeright_Gazeorigin_Y_2</th>\n",
       "      <th>Data_Eyeright_Gazeorigin_Z_2</th>\n",
       "      <th>Data_Eyeright_Gazedirection_X_2</th>\n",
       "      <th>Data_Eyeright_Gazedirection_Y_2</th>\n",
       "      <th>Data_Eyeright_Gazedirection_Z_2</th>\n",
       "      <th>Data_Eyeright_Pupildiameter_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.520701</td>\n",
       "      <td>0.331783</td>\n",
       "      <td>-18.592861</td>\n",
       "      <td>102.180206</td>\n",
       "      <td>456.542798</td>\n",
       "      <td>31.950043</td>\n",
       "      <td>-13.791783</td>\n",
       "      <td>-29.800103</td>\n",
       "      <td>-0.042355</td>\n",
       "      <td>0.202891</td>\n",
       "      <td>0.978285</td>\n",
       "      <td>3.156050</td>\n",
       "      <td>-29.904915</td>\n",
       "      <td>-10.794082</td>\n",
       "      <td>-28.784625</td>\n",
       "      <td>-0.036153</td>\n",
       "      <td>0.255270</td>\n",
       "      <td>0.966194</td>\n",
       "      <td>3.901447</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.519627</td>\n",
       "      <td>0.326055</td>\n",
       "      <td>-17.501091</td>\n",
       "      <td>105.210971</td>\n",
       "      <td>455.908248</td>\n",
       "      <td>31.947127</td>\n",
       "      <td>-13.776910</td>\n",
       "      <td>-29.800162</td>\n",
       "      <td>-0.039513</td>\n",
       "      <td>0.211895</td>\n",
       "      <td>0.976493</td>\n",
       "      <td>3.123570</td>\n",
       "      <td>-29.899446</td>\n",
       "      <td>-10.791730</td>\n",
       "      <td>-28.775490</td>\n",
       "      <td>-0.034630</td>\n",
       "      <td>0.258361</td>\n",
       "      <td>0.965428</td>\n",
       "      <td>3.934029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.519627</td>\n",
       "      <td>0.326055</td>\n",
       "      <td>-17.501091</td>\n",
       "      <td>105.210971</td>\n",
       "      <td>455.908248</td>\n",
       "      <td>31.947127</td>\n",
       "      <td>-13.776910</td>\n",
       "      <td>-29.800162</td>\n",
       "      <td>-0.039513</td>\n",
       "      <td>0.211895</td>\n",
       "      <td>0.976493</td>\n",
       "      <td>3.123570</td>\n",
       "      <td>-29.899446</td>\n",
       "      <td>-10.791730</td>\n",
       "      <td>-28.775490</td>\n",
       "      <td>-0.034630</td>\n",
       "      <td>0.258361</td>\n",
       "      <td>0.965428</td>\n",
       "      <td>3.934029</td>\n",
       "      <td>0.042314</td>\n",
       "      <td>0.516477</td>\n",
       "      <td>0.323788</td>\n",
       "      <td>-14.412214</td>\n",
       "      <td>106.361230</td>\n",
       "      <td>455.566898</td>\n",
       "      <td>32.119427</td>\n",
       "      <td>-13.811065</td>\n",
       "      <td>-29.915147</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>0.210537</td>\n",
       "      <td>0.977449</td>\n",
       "      <td>3.053843</td>\n",
       "      <td>-29.888095</td>\n",
       "      <td>-10.818350</td>\n",
       "      <td>-28.764404</td>\n",
       "      <td>-0.045779</td>\n",
       "      <td>0.264445</td>\n",
       "      <td>0.963314</td>\n",
       "      <td>3.836360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042314</td>\n",
       "      <td>0.516477</td>\n",
       "      <td>0.323788</td>\n",
       "      <td>-14.412214</td>\n",
       "      <td>106.361230</td>\n",
       "      <td>455.566898</td>\n",
       "      <td>32.119427</td>\n",
       "      <td>-13.811065</td>\n",
       "      <td>-29.915147</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>0.210537</td>\n",
       "      <td>0.977449</td>\n",
       "      <td>3.053843</td>\n",
       "      <td>-29.888095</td>\n",
       "      <td>-10.818350</td>\n",
       "      <td>-28.764404</td>\n",
       "      <td>-0.045779</td>\n",
       "      <td>0.264445</td>\n",
       "      <td>0.963314</td>\n",
       "      <td>3.836360</td>\n",
       "      <td>0.062407</td>\n",
       "      <td>0.509041</td>\n",
       "      <td>0.332158</td>\n",
       "      <td>-7.216922</td>\n",
       "      <td>101.824143</td>\n",
       "      <td>456.376223</td>\n",
       "      <td>32.365634</td>\n",
       "      <td>-13.886160</td>\n",
       "      <td>-30.283596</td>\n",
       "      <td>-0.021412</td>\n",
       "      <td>0.186305</td>\n",
       "      <td>0.982259</td>\n",
       "      <td>3.120297</td>\n",
       "      <td>-29.522238</td>\n",
       "      <td>-10.859459</td>\n",
       "      <td>-28.803975</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>0.270750</td>\n",
       "      <td>0.962560</td>\n",
       "      <td>3.777161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.062407</td>\n",
       "      <td>0.509041</td>\n",
       "      <td>0.332158</td>\n",
       "      <td>-7.216922</td>\n",
       "      <td>101.824143</td>\n",
       "      <td>456.376223</td>\n",
       "      <td>32.365634</td>\n",
       "      <td>-13.886160</td>\n",
       "      <td>-30.283596</td>\n",
       "      <td>-0.021412</td>\n",
       "      <td>0.186305</td>\n",
       "      <td>0.982259</td>\n",
       "      <td>3.120297</td>\n",
       "      <td>-29.522238</td>\n",
       "      <td>-10.859459</td>\n",
       "      <td>-28.803975</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>0.270750</td>\n",
       "      <td>0.962560</td>\n",
       "      <td>3.777161</td>\n",
       "      <td>0.082388</td>\n",
       "      <td>0.500619</td>\n",
       "      <td>0.331547</td>\n",
       "      <td>0.997201</td>\n",
       "      <td>102.017862</td>\n",
       "      <td>456.065968</td>\n",
       "      <td>32.495172</td>\n",
       "      <td>-13.895715</td>\n",
       "      <td>-30.444371</td>\n",
       "      <td>-0.019924</td>\n",
       "      <td>0.179919</td>\n",
       "      <td>0.983480</td>\n",
       "      <td>3.103205</td>\n",
       "      <td>-29.284169</td>\n",
       "      <td>-10.845078</td>\n",
       "      <td>-28.819763</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.277902</td>\n",
       "      <td>0.960450</td>\n",
       "      <td>3.992671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.082388</td>\n",
       "      <td>0.500619</td>\n",
       "      <td>0.331547</td>\n",
       "      <td>0.997201</td>\n",
       "      <td>102.017862</td>\n",
       "      <td>456.065968</td>\n",
       "      <td>32.495172</td>\n",
       "      <td>-13.895715</td>\n",
       "      <td>-30.444371</td>\n",
       "      <td>-0.019924</td>\n",
       "      <td>0.179919</td>\n",
       "      <td>0.983480</td>\n",
       "      <td>3.103205</td>\n",
       "      <td>-29.284169</td>\n",
       "      <td>-10.845078</td>\n",
       "      <td>-28.819763</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.277902</td>\n",
       "      <td>0.960450</td>\n",
       "      <td>3.992671</td>\n",
       "      <td>0.102479</td>\n",
       "      <td>0.499333</td>\n",
       "      <td>0.333044</td>\n",
       "      <td>2.242595</td>\n",
       "      <td>101.193570</td>\n",
       "      <td>456.149455</td>\n",
       "      <td>32.553355</td>\n",
       "      <td>-13.894882</td>\n",
       "      <td>-30.522387</td>\n",
       "      <td>-0.019572</td>\n",
       "      <td>0.176352</td>\n",
       "      <td>0.984133</td>\n",
       "      <td>3.093942</td>\n",
       "      <td>-29.224742</td>\n",
       "      <td>-10.866322</td>\n",
       "      <td>-28.811336</td>\n",
       "      <td>0.021887</td>\n",
       "      <td>0.278211</td>\n",
       "      <td>0.960271</td>\n",
       "      <td>4.015671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  Data_Gaze2D_X  Data_Gaze2D_Y  Data_Gaze3D_X  Data_Gaze3D_Y  Data_Gaze3D_Z  Data_Eyeleft_Gazeorigin_X  Data_Eyeleft_Gazeorigin_Y  Data_Eyeleft_Gazeorigin_Z  Data_Eyeleft_Gazedirection_X  Data_Eyeleft_Gazedirection_Y  Data_Eyeleft_Gazedirection_Z  Data_Eyeleft_Pupildiameter  Data_Eyeright_Gazeorigin_X  Data_Eyeright_Gazeorigin_Y  Data_Eyeright_Gazeorigin_Z  Data_Eyeright_Gazedirection_X  Data_Eyeright_Gazedirection_Y  Data_Eyeright_Gazedirection_Z  Data_Eyeright_Pupildiameter  Timestamp_2  Data_Gaze2D_X_2  Data_Gaze2D_Y_2  Data_Gaze3D_X_2  Data_Gaze3D_Y_2  Data_Gaze3D_Z_2  Data_Eyeleft_Gazeorigin_X_2  Data_Eyeleft_Gazeorigin_Y_2  Data_Eyeleft_Gazeorigin_Z_2  Data_Eyeleft_Gazedirection_X_2  Data_Eyeleft_Gazedirection_Y_2  Data_Eyeleft_Gazedirection_Z_2  Data_Eyeleft_Pupildiameter_2  Data_Eyeright_Gazeorigin_X_2  Data_Eyeright_Gazeorigin_Y_2  Data_Eyeright_Gazeorigin_Z_2  Data_Eyeright_Gazedirection_X_2  Data_Eyeright_Gazedirection_Y_2  Data_Eyeright_Gazedirection_Z_2  Data_Eyeright_Pupildiameter_2\n",
       "0   0.002242       0.520701       0.331783     -18.592861     102.180206     456.542798                  31.950043                 -13.791783                 -29.800103                     -0.042355                      0.202891                      0.978285                    3.156050                  -29.904915                  -10.794082                  -28.784625                      -0.036153                       0.255270                       0.966194                     3.901447     0.022335         0.519627         0.326055       -17.501091       105.210971       455.908248                    31.947127                   -13.776910                   -29.800162                       -0.039513                        0.211895                        0.976493                      3.123570                    -29.899446                    -10.791730                    -28.775490                        -0.034630                         0.258361                         0.965428                       3.934029\n",
       "1   0.022335       0.519627       0.326055     -17.501091     105.210971     455.908248                  31.947127                 -13.776910                 -29.800162                     -0.039513                      0.211895                      0.976493                    3.123570                  -29.899446                  -10.791730                  -28.775490                      -0.034630                       0.258361                       0.965428                     3.934029     0.042314         0.516477         0.323788       -14.412214       106.361230       455.566898                    32.119427                   -13.811065                   -29.915147                       -0.016369                        0.210537                        0.977449                      3.053843                    -29.888095                    -10.818350                    -28.764404                        -0.045779                         0.264445                         0.963314                       3.836360\n",
       "2   0.042314       0.516477       0.323788     -14.412214     106.361230     455.566898                  32.119427                 -13.811065                 -29.915147                     -0.016369                      0.210537                      0.977449                    3.053843                  -29.888095                  -10.818350                  -28.764404                      -0.045779                       0.264445                       0.963314                     3.836360     0.062407         0.509041         0.332158        -7.216922       101.824143       456.376223                    32.365634                   -13.886160                   -30.283596                       -0.021412                        0.186305                        0.982259                      3.120297                    -29.522238                    -10.859459                    -28.803975                        -0.013163                         0.270750                         0.962560                       3.777161\n",
       "3   0.062407       0.509041       0.332158      -7.216922     101.824143     456.376223                  32.365634                 -13.886160                 -30.283596                     -0.021412                      0.186305                      0.982259                    3.120297                  -29.522238                  -10.859459                  -28.803975                      -0.013163                       0.270750                       0.962560                     3.777161     0.082388         0.500619         0.331547         0.997201       102.017862       456.065968                    32.495172                   -13.895715                   -30.444371                       -0.019924                        0.179919                        0.983480                      3.103205                    -29.284169                    -10.845078                    -28.819763                         0.017489                         0.277902                         0.960450                       3.992671\n",
       "4   0.082388       0.500619       0.331547       0.997201     102.017862     456.065968                  32.495172                 -13.895715                 -30.444371                     -0.019924                      0.179919                      0.983480                    3.103205                  -29.284169                  -10.845078                  -28.819763                       0.017489                       0.277902                       0.960450                     3.992671     0.102479         0.499333         0.333044         2.242595       101.193570       456.149455                    32.553355                   -13.894882                   -30.522387                       -0.019572                        0.176352                        0.984133                      3.093942                    -29.224742                    -10.866322                    -28.811336                         0.021887                         0.278211                         0.960271                       4.015671"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: fix reading of files. i think it is bugged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
